{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d8e4ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,LSTM, SpatialDropout1D, Embedding, Reshape, Concatenate\n",
    "from keras.layers import Flatten\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5136201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "OUTPUT_SIZE = 20\n",
    "INPUT_SIZE = 60\n",
    "HIDEN_SIZE = 20\n",
    "IMAGE_DATA_SZ = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14587d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\"There is no focal consolidation, pleural effusion or pneumothorax. Bilateral nodular opacities that most likely represent nipple shadows. The cardiomediastinal silhouette is normal. Clips project over the left lung, potentially within the breast. The imaged upper abdomen is unremarkable. Chronic deformity of the posterior left sixth and seventh ribs are noted.\", \n",
    "     \"The cardiac, mediastinal and hilar contours are normal. Pulmonary vasculature is normal. Lungs are clear. No pleural effusion or pneumothorax is present. Multiple clips are again seen projecting over the left breast. Remote left-sided rib fractures are also re- demonstrated. Chexpert Prediction is No Finding\",\n",
    "    \"Single frontal view of the chest provided. There is no focal consolidation, effusion, or pneumothorax. The cardiomediastinal silhouette is normal. Again seen are multiple clips projecting over the left breast and remote left-sided rib fractures. No free air below the right hemidiaphragm is seen.\",\n",
    "    \"The lungs are clear of focal consolidation, pleural effusion or pneumothorax. The heart size is normal. The mediastinal contours are normal. Multiple surgical clips project over the left breast, and old left rib fractures are noted. Chexpert Prediction is No Finding\",\n",
    "    \"PA and lateral views of the chest provided. The lungs are adequately aerated. There is a focal consolidation at the left lung base adjacent to the lateral hemidiaphragm. There is mild vascular engorgement. There is bilateral apical pleural thickening. The cardiomediastinal silhouette is remarkable for aortic arch calcifications. The heart is top normal in size. Chexpert Prediction is Lung Opacity\"]\n",
    "\n",
    "Y_train = [\"No acute cardiopulmonary process.\", \"No acute cardiopulmonary abnormality.\",\"No acute intrathoracic process.\",\"No acute cardiopulmonary process.\",\"Focal consolidation at the left lung base, possibly representing aspiration or pneumonia. Central vascular engorgement.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a69a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eea39ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>findings</th>\n",
       "      <th>impression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is no focal consolidation, pleural effus...</td>\n",
       "      <td>No acute cardiopulmonary process.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The cardiac, mediastinal and hilar contours ar...</td>\n",
       "      <td>No acute cardiopulmonary abnormality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Single frontal view of the chest provided. The...</td>\n",
       "      <td>No acute intrathoracic process.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The lungs are clear of focal consolidation, pl...</td>\n",
       "      <td>No acute cardiopulmonary process.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PA and lateral views of the chest provided. Th...</td>\n",
       "      <td>Focal consolidation at the left lung base, pos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            findings  \\\n",
       "0  There is no focal consolidation, pleural effus...   \n",
       "1  The cardiac, mediastinal and hilar contours ar...   \n",
       "2  Single frontal view of the chest provided. The...   \n",
       "3  The lungs are clear of focal consolidation, pl...   \n",
       "4  PA and lateral views of the chest provided. Th...   \n",
       "\n",
       "                                          impression  \n",
       "0                  No acute cardiopulmonary process.  \n",
       "1              No acute cardiopulmonary abnormality.  \n",
       "2                    No acute intrathoracic process.  \n",
       "3                  No acute cardiopulmonary process.  \n",
       "4  Focal consolidation at the left lung base, pos...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8073e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = df['findings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ccabadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train = df['impression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd825397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbbb3550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "corpus = [gensim.utils.simple_preprocess(sentence) for sentence in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f81829bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42562e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=100,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=4-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3bc035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(corpus, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76907d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word vectors for your input sentences\n",
    "train_vectors = []\n",
    "embedding_size = 100\n",
    "for sentence in corpus:\n",
    "    sentence_vectors = []\n",
    "    for word in sentence:\n",
    "        if word in model.wv:\n",
    "            sentence_vectors.append(tf.convert_to_tensor(np.array(model.wv[word])))\n",
    "        else:\n",
    "            # Handle out-of-vocabulary words\n",
    "            sentence_vectors.append(tf.convert_to_tensor(np.array([0]*embedding_size)))\n",
    "    while(len(sentence_vectors)<INPUT_SIZE):\n",
    "        sentence_vectors.append(np.zeros(100))\n",
    "    train_vectors.append(tf.convert_to_tensor(np.array(sentence_vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f617cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = np.array(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b13b17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [gensim.utils.simple_preprocess(sentence) for sentence in Y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "930b06a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word vectors for your input sentences\n",
    "test_vectors = []\n",
    "embedding_size = 100\n",
    "for sentence in test_corpus:\n",
    "    sentence_vectors = []\n",
    "    for word in sentence:\n",
    "        if word in model.wv:\n",
    "            sentence_vectors.append(tf.convert_to_tensor(np.array(model.wv[word])))\n",
    "        else:\n",
    "            # Handle out-of-vocabulary words\n",
    "            sentence_vectors.append(tf.convert_to_tensor(np.array([0]*embedding_size)))\n",
    "    while(len(sentence_vectors)<OUTPUT_SIZE):\n",
    "        sentence_vectors.append(np.zeros(100))\n",
    "    test_vectors.append(tf.convert_to_tensor(np.array(sentence_vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c81b8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_vectors = np.array(test_vectors).reshape(len(test_vectors),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "172d49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_vectors = tf.convert_to_tensor(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fcecdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([20, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ed963b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "temp = np.array(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4868a8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1e81da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20d52948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "temp = np.array(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e0b573f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7cb5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d7b4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# extra_data = np.zeros((5, IMAGE_DATA_SZ))\n",
    "\n",
    "# # Define the model architecture\n",
    "# model_nn = Sequential()\n",
    "# model_nn.add(Flatten(input_shape=(60, 100)))\n",
    "# model_nn.add(Dense(INPUT_SIZE//2, activation='relu', input_dim=INPUT_SIZE))\n",
    "# model_nn.add(Dense(INPUT_SIZE//4, activation='relu'))\n",
    "\n",
    "# # # Generate a random numpy array\n",
    "# #random_array = np.random.rand(1024)\n",
    "\n",
    "# # # Define a Lambda layer to concatenate the random array with the output of the second layer\n",
    "# # concat_layer = Concatenate()([model_nn.layers[1].output, Lambda(lambda x: random_array)(model_nn.layers[1].output)])\n",
    "\n",
    "# # # Add the concatenation layer to the model\n",
    "# #model_nn.add(Dense(1024, activation='relu')(concat_layer))\n",
    "# ## Add image 1024 data\n",
    "# model_nn.add(Concatenate(axis=0))\n",
    "# #model_nn.add(Dense(INPUT_SIZE//4+IMAGE_DATA_SZ, activation='relu'))\n",
    "# model_nn.add(Dense(2000//4, activation='relu'))\n",
    "# model_nn.add(Dense(2000//2, activation='relu'))\n",
    "# model_nn.add(Dense(2000, activation='sigmoid'))\n",
    "# model_nn.add(Reshape((20, 100)))\n",
    "\n",
    "\n",
    "# # Compile the model\n",
    "# model_nn.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# # Train the model on some data\n",
    "# model_nn.fit(train_vectors, test_vectors, epochs=100, batch_size=5, validation_data=(train_vectors, test_vectors))\n",
    "\n",
    "# # Evaluate the model on some test data\n",
    "# loss, accuracy = model_nn.evaluate(train_vectors, test_vectors)\n",
    "\n",
    "# # Make predictions using the model\n",
    "# predictions = model_nn.predict(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1227b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(predictions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "009b4ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "105925f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_embeddings_to_sentences(glove_embeddings, model):\n",
    "    # Find the closest words to each GloVe embedding vector\n",
    "    sentences = []\n",
    "    for glove_embedding in glove_embeddings:\n",
    "        words = []\n",
    "        for embedding in glove_embedding:\n",
    "            print(model.wv.similar_by_vector(embedding, topn=1))\n",
    "            try:\n",
    "                words.append(model.wv.similar_by_vector(embedding, topn=1)[0][0])\n",
    "            except:\n",
    "                print(model.wv.similar_by_vector(embedding, topn=1))\n",
    "                words.append(\" \")\n",
    "\n",
    "        # Convert the words to a sentence\n",
    "        sentence = \" \".join(words)\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fab7c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove_embeddings_to_sentences(predictions, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7494103",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d02d9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.layers import Concatenate, Input\n",
    "\n",
    "# input_shape = (60, 100)\n",
    "\n",
    "# # create a numpy array of size (batch_size, 1024)\n",
    "# extra_data = np.zeros((batch_size, 1024))\n",
    "# extra_data_shape = (1024,)\n",
    "\n",
    "# input_layer = Input(shape=input_shape)\n",
    "\n",
    "# x = Flatten()(input_layer)\n",
    "# x = Dense(INPUT_SIZE//2, activation='relu', input_dim=INPUT_SIZE)(x)\n",
    "# x = Dense(INPUT_SIZE//4, activation='relu')(x)\n",
    "# # create the extra data input layer\n",
    "# extra_data_layer = Input(shape=extra_data_shape)\n",
    "\n",
    "# x = Concatenate()([x, extra_data_layer])\n",
    "# x = Dense(2000//4, activation='relu')(x)\n",
    "# x = Dense(2000//2, activation='relu')(x)\n",
    "# x = Dense(2000, activation='sigmoid')(x)\n",
    "# output_layer = Reshape((20, 100))(x)\n",
    "# model_nn = Model(inputs=[input_layer, extra_data_layer], outputs=output_layer)\n",
    "\n",
    "\n",
    "# # model_nn = Sequential()\n",
    "# # model_nn.add(Flatten(input_shape=(60, 100)))\n",
    "# # model_nn.add(Dense(INPUT_SIZE//2, activation='relu', input_dim=INPUT_SIZE))\n",
    "# # model_nn.add(Dense(INPUT_SIZE//4, activation='relu'))\n",
    "\n",
    "# # # concatenate the extra data array\n",
    "# # model_nn.add(Concatenate(axis=1))\n",
    "\n",
    "# # # add another layer\n",
    "# # model_nn.add(Dense(128, activation='relu'))\n",
    "# # model_nn.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08234c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "#model_nn.compile(optimizer='adam', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b10de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e45c0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on some data\n",
    "# model_nn.fit([train_vectors,np.zeros((batch_size, 1024))], test_vectors, epochs=100, batch_size=5, validation_data=(train_vectors, test_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ccce48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 789ms/step - loss: 0.2570 - accuracy: 0.0100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.2594 - accuracy: 0.0600\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2611 - accuracy: 0.0200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2623 - accuracy: 0.0100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2621 - accuracy: 0.0100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2632 - accuracy: 0.0200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2663 - accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2638 - accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2650 - accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2631 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Concatenate, Input, Dense, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "# define the input shapes\n",
    "input_shape = (60, 100)\n",
    "extra_data_shape = (1024,)\n",
    "\n",
    "# create a numpy array of size (batch_size, 1024)\n",
    "batch_size = 5\n",
    "extra_data = np.zeros((batch_size, 1024))\n",
    "\n",
    "# create the input layers\n",
    "input_layer = Input(shape=input_shape)\n",
    "extra_data_layer = Input(shape=extra_data_shape)\n",
    "\n",
    "# add the layers to the model\n",
    "x = Flatten()(input_layer)\n",
    "x = Dense(INPUT_SIZE//3, activation='relu', input_dim=INPUT_SIZE)(x)\n",
    "x = Dense(INPUT_SIZE//9, activation='relu')(x)\n",
    "x = Dense(INPUT_SIZE//27, activation='relu')(x)\n",
    "x = Dense(INPUT_SIZE//81, activation='relu')(x)\n",
    "# x = Dense(INPUT_SIZE//243, activation='relu')(x)\n",
    "# x = Dense(INPUT_SIZE//729, activation='relu')(x)\n",
    "\n",
    "\n",
    "\n",
    "# concatenate the extra data with the output of the last layer\n",
    "x = Concatenate()([x, extra_data_layer])\n",
    "\n",
    "# x = Dense(2000//729, activation='relu')(x)\n",
    "# x = Dense(2000//243, activation='relu')(x)\n",
    "x = Dense(2000//81, activation='relu')(x)\n",
    "x = Dense(2000//27, activation='relu')(x)\n",
    "x = Dense(2000//9, activation='relu')(x)\n",
    "x = Dense(2000//3, activation='sigmoid')(x)\n",
    "x = Dense(2000, activation='sigmoid')(x)\n",
    "output_layer = Reshape((20, 100))(x)\n",
    "\n",
    "# create the model\n",
    "model_nn = Model(inputs=[input_layer, extra_data_layer], outputs=output_layer)\n",
    "\n",
    "# compile the model\n",
    "model_nn.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# create some dummy input data\n",
    "input_data = np.random.rand(batch_size, 60, 100)\n",
    "\n",
    "# train the model for 10 epochs\n",
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    # create some dummy target data\n",
    "    target_data = np.random.randint(2, size=(batch_size, 20, 100))\n",
    "    # train the model for one epoch\n",
    "    model_nn.fit([train_vectors, extra_data], target_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf1452b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f2e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
